{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7cccbcd-aff7-4471-969a-000a2e960237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf294a26-db3e-48a8-b0ae-e7ba62939fea",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02225f6b-ff5c-428d-b541-4f38fe92f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # 1. Handle zero-width joiner\n",
    "    sentence = re.sub(\"\\u200c\", \" \", sentence)\n",
    "\n",
    "    # 2. Replace URLs (http, https, www)\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '<URL>', sentence)\n",
    "\n",
    "    # 3. Replace numbers (any continuous digits)\n",
    "    sentence = re.sub(r'\\d+', '<NUMBER>', sentence)\n",
    "\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' <PUNCT> ', sentence)\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa32bb4b-3cfa-4f94-bbf4-e01d3e8f3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_with_normalization(sentence, smoothing=False):\n",
    "    TF = {}\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            TF[word] += 1\n",
    "        except:\n",
    "            TF[word] = 1\n",
    "\n",
    "    length = len(sentence)\n",
    "\n",
    "    if not smoothing:\n",
    "        for key in TF.keys():\n",
    "            TF[key] /= length\n",
    "\n",
    "    else:\n",
    "        denom = 0\n",
    "        for key in TF.keys():\n",
    "            denom += (1 + math.log(TF[key]))\n",
    "\n",
    "        for key in TF.keys():\n",
    "            TF[key] /= denom\n",
    "\n",
    "    return TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a460c2b-f834-4d11-a4ef-37327fa1fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(sentence, sentences, smoothing=False):\n",
    "    IDF = {}\n",
    "    N = len(sentences)\n",
    "    for word in sentence:\n",
    "        IDF[word] = 0\n",
    "        for s in sentences:\n",
    "            if word in s:\n",
    "                IDF[word] += 1\n",
    "\n",
    "    if not smoothing:\n",
    "        for key in IDF.keys():\n",
    "            IDF[key] = math.log(N/IDF[key])\n",
    "\n",
    "    else:\n",
    "        for key in IDF.keys():\n",
    "            IDF[key] = math.log(((1+N)/(1+IDF[key]))) + 1\n",
    "\n",
    "    return IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8942d9a9-b642-4581-a76f-01bb96e8e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf_scores(sentences, smoothing=False):\n",
    "    TF_IDF = {}\n",
    "    for sentence in sentences:\n",
    "        List = []\n",
    "        TF = compute_tf_with_normalization(sentence, smoothing)\n",
    "        IDF = compute_idf(sentence, sentences, smoothing)\n",
    "        for key in TF.keys():\n",
    "            List.append(TF[key]*IDF[key])\n",
    "\n",
    "        TF_IDF[sentence] = List\n",
    "\n",
    "    return TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7075dcd-ab78-4dd7-8a1b-43bbacb9fe13",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "356450b2-01a6-4e84-8e08-3bc572973057",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The boy hugs the cat.\",\n",
    "    \"The boys are hugging the dogs.\",\n",
    "    \"The dogs are chasing the cats.\",\n",
    "    \"The dog and the cat sit quietly.\",\n",
    "    \"The boy is sitting on the dog.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c99014-d158-4ec6-b832-a68c9f8c2424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wordpiece(sentences, iteration = 20):\n",
    "    for sentence in range(len(sentences)):\n",
    "        sentences[sentence] = preprocess(sentences[sentence])\n",
    "\n",
    "        for word in range(len(sentences[sentence])):\n",
    "            if \"<\" in sentences[sentence][word]:\n",
    "                sentences[sentence][word] = [sentences[sentence][word]]\n",
    "                continue\n",
    "\n",
    "            sentences[sentence][word] = list(sentences[sentence][word])\n",
    "\n",
    "    corpus = [word for sentence in sentences for word in sentence]\n",
    "\n",
    "    for it in range(iteration):\n",
    "        pairs = {}\n",
    "        for word in corpus:\n",
    "            for i in range(len(word)-1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += 1\n",
    "                else:\n",
    "                    pairs[pair] = 1\n",
    "\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "\n",
    "        best_pair = None\n",
    "        max_count = -1\n",
    "        for pair in pairs:\n",
    "            if pairs[pair] > max_count:\n",
    "                best_pair = pair\n",
    "                max_count = pairs[pair]\n",
    "\n",
    "\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word)-1 and (word[i], word[i+1]) == best_pair:\n",
    "                    new_word.append(word[i] + word[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        corpus = new_corpus\n",
    "\n",
    "    vocab = []\n",
    "    for word in corpus:\n",
    "        for token in word:\n",
    "            if token not in vocab:\n",
    "                vocab.append(token)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c95f046-75e5-4c3c-8f69-c7850441cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence, vocab):\n",
    "\n",
    "    List = []\n",
    "    for word in sentence:\n",
    "        if \"<\" in word:  # special tokens\n",
    "            List.append(word)\n",
    "            continue\n",
    "\n",
    "        w = list(word)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(w):\n",
    "\n",
    "            match = None\n",
    "            for j in range(len(w), i, -1):\n",
    "                candidate = ''.join(w[i:j])\n",
    "                if candidate in vocab:\n",
    "                    match = candidate\n",
    "                    w = w[:i] + [candidate] + w[j:]\n",
    "                    i += 1\n",
    "                    break\n",
    "\n",
    "            if match is None:\n",
    "                i += 1\n",
    "\n",
    "        List.extend(w)\n",
    "\n",
    "    return List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05eba0b9-ee38-4fc9-810d-d60fb290191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The cat is chasing the dog quietly.'\n",
    "sentence = preprocess(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f398967-5a30-433e-8f38-b2a450153f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The boy hugs the cat.',\n",
       " 'The boys are hugging the dogs.',\n",
       " 'The dogs are chasing the cats.',\n",
       " 'The dog and the cat sit quietly.',\n",
       " 'The boy is sitting on the dog.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aa80334-3fda-4908-aa13-801bc8381286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'i', 's', 'c', 'h', 'a', 's', 'ing', 'the', 'dog', 'q', 'u', 'i', 'e', 't', 'l', 'y', '<punct>']\n"
     ]
    }
   ],
   "source": [
    "v = Wordpiece(sentences)\n",
    "print(Tokenize(sentence, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323e903a-05dc-4cff-8054-1a1ead6f2ee9",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2c8b3ef-d522-4daa-8543-5adf76224d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NGram(Gram: int = 1, Paragraph: list = None, Smoothing = None):\n",
    "    L_List = []\n",
    "    G_List = []\n",
    "\n",
    "    if Paragraph is None:\n",
    "        return\n",
    "\n",
    "    # --- Unigrams ---\n",
    "    if Gram >= 1:\n",
    "        Gram_1 = {}\n",
    "        total = 0\n",
    "        for sentence in Paragraph:\n",
    "            for word in sentence:\n",
    "                Gram_1[word] = Gram_1.get(word, 0) + 1\n",
    "                total += 1\n",
    "\n",
    "        Count_1 = copy.deepcopy(Gram_1)\n",
    "        for k in Gram_1:\n",
    "            Gram_1[k] = Gram_1[k] / total\n",
    "\n",
    "        L_List.append(Count_1)\n",
    "        G_List.append(Gram_1)\n",
    "\n",
    "    # --- Bigrams ---\n",
    "    if Gram >= 2:\n",
    "        Gram_2 = {}\n",
    "        for sentence in Paragraph:\n",
    "            for i in range(len(sentence)-1):\n",
    "                s = tuple(sentence[i:i+2])\n",
    "                Gram_2[s] = Gram_2.get(s, 0) + 1\n",
    "\n",
    "        Count_2 = copy.deepcopy(Gram_2)\n",
    "        for k in Gram_2:\n",
    "            Gram_2[k] = Gram_2[k] / Count_1.get(k[0], 1)\n",
    "\n",
    "        L_List.append(Count_2)\n",
    "        G_List.append(Gram_2)\n",
    "\n",
    "    # --- Trigrams ---\n",
    "    if Gram >= 3:\n",
    "        Gram_3 = {}\n",
    "        for sentence in Paragraph:\n",
    "            for i in range(len(sentence)-2):\n",
    "                s = tuple(sentence[i:i+3])\n",
    "                Gram_3[s] = Gram_3.get(s, 0) + 1\n",
    "\n",
    "        Count_3 = copy.deepcopy(Gram_3)\n",
    "        for k in Gram_3:\n",
    "            Gram_3[k] = Gram_3[k] / Count_2.get(k[:2], 1)\n",
    "\n",
    "        L_List.append(Count_3)\n",
    "        G_List.append(Gram_3)\n",
    "\n",
    "    # --- Quadgrams ---\n",
    "    if Gram >= 4:\n",
    "        Gram_4 = {}\n",
    "        for sentence in Paragraph:\n",
    "            for i in range(len(sentence)-3):\n",
    "                s = tuple(sentence[i:i+4])\n",
    "                Gram_4[s] = Gram_4.get(s, 0) + 1\n",
    "\n",
    "        Count_4 = copy.deepcopy(Gram_4)\n",
    "        for k in Gram_4:\n",
    "            Gram_4[k] = Gram_4[k] / Count_3.get(k[:3], 1)\n",
    "\n",
    "        L_List.append(Count_4)\n",
    "        G_List.append(Gram_4)\n",
    "\n",
    "    # --- Apply Smoothing if Provided ---\n",
    "    if Smoothing is not None:\n",
    "        L1, G1 = L_List[0], G_List[0]\n",
    "        L2 = L3 = L4 = None\n",
    "        G2 = G3 = G4 = None\n",
    "        if Gram >= 2:\n",
    "            L2, G2 = L_List[1], G_List[1]\n",
    "        if Gram >= 3:\n",
    "            L3, G3 = L_List[2], G_List[2]\n",
    "        if Gram >= 4:\n",
    "            L4, G4 = L_List[3], G_List[3]\n",
    "\n",
    "        Smoothing(Gram=Gram, L1=L1, L2=L2, L3=L3, L4=L4,\n",
    "                  G1=G1, G2=G2, G3=G3, G4=G4, Data=None)\n",
    "\n",
    "    return L_List, G_List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78e8aad8-688d-4824-88c9-a5596a87e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddK_Smoothing(Gram=1, L1=None, L2=None, L3=None, L4=None, G1=None, G2=None, G3=None, G4=None, Data=None, K=0.3):\n",
    "\n",
    "    if Gram >= 1:\n",
    "        total_count = sum(L1.values())\n",
    "        V = len(L1)  # vocabulary size\n",
    "        for word in L1:\n",
    "            G1[word] = (L1[word] + K) / (total_count + K * V)\n",
    "\n",
    "    if Gram >= 2 and L2 is not None:\n",
    "        for bigram in L2:\n",
    "            history = bigram[0]\n",
    "            history_count = L1.get(history, 0)\n",
    "            G2[bigram] = (L2[bigram] + K) / (history_count + K * len(L1))\n",
    "\n",
    "    if Gram >= 3 and L3 is not None:\n",
    "        for trigram in L3:\n",
    "            history = trigram[:2]\n",
    "            history_count = L2.get(history, 0)\n",
    "            G3[trigram] = (L3[trigram] + K) / (history_count + K * len(L1))\n",
    "\n",
    "    if Gram >= 4 and L4 is not None:\n",
    "        for quadgram in L4:\n",
    "            history = quadgram[:3]\n",
    "            history_count = L3.get(history, 0)\n",
    "            G4[quadgram] = (L4[quadgram] + K) / (history_count + K * len(L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1545cc6b-839b-4306-a82f-2ccb077320fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform = [\"Check out https://example.com for more info!\", \"Your package #12345 will arrive tomorrow.\", \"Download the report from https://reports.com.\"]\n",
    "Reminder = [\"Meeting at 3pm, don't forget to bring the files.\", \"The meeting is starting in 10 minutes.\", \"Reminder: submit your timesheet by 5pm today.\"]\n",
    "Promo = [\"Order 3 items, get 1 free! Limited offer!!!\", \"Win $1000 now, visit http://winbig.com!!!\", \"Exclusive deal for you: buy 2, get 1 free!!!\"]\n",
    "\n",
    "Inform = [preprocess(s) for s in Inform]\n",
    "Reminder = [preprocess(s) for s in Reminder]\n",
    "Promo = [preprocess(s) for s in Promo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "874d703d-1d7f-4344-bd6d-9d1a9c876139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inform_Count, Inform_Probability = NGram(2, Inform, AddK_Smoothing)\n",
    "Reminder_Count, Reminder_Probability = NGram(2, Reminder, AddK_Smoothing)\n",
    "Promo_Count, Promo_Probability = NGram(2, Promo, AddK_Smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9c85cf9-b27a-4302-83bc-0e86ae5b0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_probability(sentence, Count_dict, K=0.3):\n",
    "    Count_bigram = Count_dict[0]  # bigram counts\n",
    "    Count_unigram = Count_dict[1]  # unigram counts\n",
    "\n",
    "    prob = 1.0\n",
    "\n",
    "    # generate bigrams from sentence\n",
    "    bigrams = [tuple(sentence[i:i+2]) for i in range(len(sentence)-1)]\n",
    "\n",
    "    # Vocabulary size for Add-K smoothing\n",
    "    V = len(Count_unigram)\n",
    "\n",
    "    for bg in bigrams:\n",
    "        bg_count = Count_bigram.get(bg, 0)\n",
    "        history_count = Count_unigram.get(bg[0], 0)\n",
    "\n",
    "        # Add-K smoothing formula\n",
    "        prob_bg = (bg_count + K) / (history_count + K*V)\n",
    "        prob *= prob_bg\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1b40743-8b1a-4688-a20c-e03b9a192a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"You will get an exclusive offer in the meeting!\"\n",
    "test_tokens = preprocess(test_sentence)\n",
    "\n",
    "test_bigrams = [tuple(test_tokens[i:i+2]) for i in range(len(test_tokens)-1)]\n",
    "\n",
    "prob_Inform = sentence_probability(test_bigrams, Inform_Probability)\n",
    "prob_Reminder = sentence_probability(test_bigrams, Reminder_Probability)\n",
    "prob_Promo = sentence_probability(test_bigrams, Promo_Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27fb04f2-8ad2-43db-a6de-9da1649c3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inform: 2.6439037579245584e-11 | Reminder: 1.4989812156737662e-12 | Promotion: 4.7886513275010046e-12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inform: {prob_Inform} | Reminder: {prob_Reminder} | Promotion: {prob_Promo}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
